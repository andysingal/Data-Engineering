# Data-Engineering (Coursera IBM Data Engineering Course)

Project Overview
Scenario
For this project, you will assume the role of data engineer working for an international financial analysis company. Your company tracks stock prices, commodities, forex rates, inflation rates.  Your job is to extract financial data from various sources like websites, APIs and files provided by various financial analysis firms. After you collect the data, you extract the data of interest to your company and transform it based on the requirements given to you. Once the transformation is complete you load that data into a database.

Project Tasks
In this project you will:

Collect data using APIs

Collect data using webscraping.

Download files to process.    

Read csv, xml and json file types.

Extract data from the above file types.

Transform data.

Use the built in logging module.

Save the transformed data in a ready-to-load format which data engineers can use to load the data.

![11](https://github.com/andysingal/Data-Engineering/blob/main/Images/Screenshot%202023-07-11%20at%206.53.42%20PM.png)

![12](https://github.com/andysingal/Data-Engineering/blob/main/Images/Screenshot%202023-07-17%20at%201.36.47%20PM.png)


The following are some considerations for choosing data technologies across the data engineering lifecycle:

- Team size and capabilities: Take an inventory of your teamâ€™s skills. Do people lean toward low-code tools, or do they favor code-first approaches? Are people strong in certain languages like Java, Python, or Go? Technologies are available to cater to every preference on the low-code to code-heavy spectrum. 

- Speed to market: Speed to market is crucial in technology. Choose the right technologies, deliver value early, and iterate for continuous improvement. Avoid overthinking and prioritize familiar tools for efficiency. Slow decisions and output can lead to failure; aim for rapid, reliable, and secure operations to succeed.

- Interoperability: Interoperability is crucial when choosing technologies, as they often need to connect and exchange information. Seamlessly integrating products ensures easy setup, while manual configurations may require more effort. Data tools usually have built-in integrations with popular platforms, and standards like JDBC or ODBC enable easy database connections. REST APIs vary, requiring careful integration by vendors or open-source projects. Designing for modularity allows flexibility in swapping technologies as new practices emerge, so consider ease of connecting technologies throughout the data engineering lifecycle. Prioritize interoperability to ensure efficient and effective technology interactions.

- Cost optimization and business value: Interoperability is essential; technologies should connect and exchange information efficiently.
Consider costs through total cost of ownership (TCO) and differentiate between capital expenses (capex) and operational expenses (opex).
Opex allows flexibility and cost-effectiveness, especially in cloud-based services, promoting an opex-first approach.
Total opportunity cost of ownership (TOCO) should be evaluated, as choices may exclude other possibilities and hinder future adaptability.
FinOps focuses on financial accountability, enabling quick iterations and dynamic scalability, ultimately driving business value in data engineering.

- Today versus the future: immutable versus transitory technologies: 

- Location (cloud, on prem, hybrid cloud, multicloud)

- Build versus buy

- Monolith versus modular

- Serverless versus servers

- Optimization, performance, and the benchmark wars

- The undercurrents of the data engineering lifecycle
